{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size: 300%; font-weight: bold; color: maroon;\" align=\"center\">Comparación de distintos modelos<br><br>para la predicción de Series Temporales</div>\n",
    "<br><div style=\"font-size: 200%; font-weight: bold; color: maroon;\" align=\"center\">Un enfoque híbrido en Python y R</div>\n",
    "\n",
    "<!-- Alejandro Valladares Vaquero, v. 1.0, 2019 -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Índice\n",
    "* [1 - Introducción](#1)\n",
    "* [2 - Ingestión de datos y limpieza del dataset](#2)\n",
    "* [3 - Comprobación de la estacionariedad de la serie (hipótesis de raíz unitaria) - R](#3)\n",
    "* [4 - División en conjunto de entrenamiento y validación](#4)\n",
    "* [5 - Método I: Holt-Winters con triple suavizamiento exponencial](#5)\n",
    "* [6 - Método II: Predicción utilizando la librería **Prophet** de Facebook](#6)\n",
    "* [7 - Método III: Métodos de predicción simples. Implementación en R](#7)\n",
    "* [8 - Método IV: Regresión armónica dinámica (DHR) para estacionalidades múltiples - Implementado en R](#8)\n",
    "* [9 - Método V: TBATS](#9)\n",
    "* [10 - Conclusiones](#10)\n",
    "* [11 - Futuras líneas de trabajo](#11)\n",
    "* [12 - Referencias](#12)\n",
    "* [Anexo: Validación cruzada anidada para series temporales](#anexo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSS Style cell for the Notebook Output:\n",
    "# from IPython.display import display, HTML\n",
    "\n",
    "# CSS = \"\"\"\n",
    "# .output {\n",
    "#     align-items: center;\n",
    "# }\n",
    "# \"\"\"\n",
    "\n",
    "# HTML('<style>{}</style>'.format(CSS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name=\"1\"></a>1 - Introducción\n",
    "\n",
    "Se trada de un subconjunto de datos numéricos, entre los que existe una relación temporal.\n",
    "\n",
    "Las series temporales necesitan su propia metodología de análisis, ya que la componente temporal añade un orden de complejidad y autocorrelación entre los datos de una misma variable.\n",
    "\n",
    "Igual que en datos estacionarios se buscan patrones \"ocultos\" en los datos, ya sea entre variables del data set o variables creadas mediante proceso de data engineering. Para las series temporales se buscan la existencia de estacionalidad, análisis de tendencias o fluctuaciones periódicas (ciclos).\n",
    "\n",
    "Existen diversos algoritmos que se pueden utilizar para analizar series temporales. Desde regresiones lineales sencillas, para hacer análisis de tendencia más sencillos, regresión mediante árboles de decisión, Random Forest o Gradient Boosting Methods o modelos ARIMA (AutoRegressive Integrated Moving Average). También existen modelos más complejos que pueden llegar a capturar las relaciones no lineales de series temporales complejas, como pueden ser las redes neuronales LSTM (Long-short Term Memory Networks), TDNN (Time Delay Neural Networks)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"1.1\"></a> 1.1 - Dataset de Kaggle: *Hourly Energy Consumption*\n",
    "\n",
    "El dataset puede descargarse de Kaggle en la siguiente URL:\n",
    "https://www.kaggle.com/robikscube/hourly-energy-consumption#NI_hourly.csv\n",
    "\n",
    "Se trata de una serie temporal univariante, es decir, que sólo aporta datos históricos de una sóla variable. \n",
    "En este caso se trata de una estimación del consumo eléctrico en Mega Watios (MW) donado por la compañia PJM Interconection LLC. Se trata de una compañía de distribución eléctrica regional en los Estados Unidos. Se encarga de distribuir electricidad entre varios estados de la zona noreste y medio este del país (algunos de ellos, Delaware, Illinois, Indiana, Kentucky, Maryland, Michigan, New Jersey, Ohio, Pennsylvania, Tennessee, Virginia, etc.)\n",
    "\n",
    "## <a name=\"1.2\"></a> 1.2 - Análisis\n",
    "\n",
    "El trabajo se ha realizado utilizando tanto Python como R. En Python se ha llevado a cabo la mayoría del EDA aprovechando la flexibilidad y potencia de la librería pandas, la mayoríad de los gráficos se renderizan en **plotly**, ya que la interactividad ha resultado muy útil en el contexto del análisis de series. También se ha recurrido a **matplotlib** y **seaborn** para algunas de las visualizaciones. El análisis de la serie temporal se ha llevado a cabo con la librería **statsmodel** y la librería **Prophet** de Facebook.\n",
    "\n",
    "R es conocido por su capacidad a la hora de hacer análisis y predicción de series temporales con los objetos **ts**. Para el análisis de los datos se ha optado por utilizar la librería **forecast**, que a su vez se apoya en otros paquetes vistos durante el curso como **ggplot**.\n",
    "\n",
    "El trabajo se ha desarrollado de la siguiente forma:\n",
    "\n",
    "- **Sección 2.** Ingestión de datos y limpieza (eliminación de valores duplicados, creación del índice temporal, etc)\n",
    "<br><br>\n",
    "- **Sección 2.1.** EDA: Búsqueda de patrones estacionales, descomposición de la serie en tendecia, estacionalidad y residuos\n",
    "<br><br>\n",
    "- **Sección 3.** Comprobación formal de la estacionalidad de la serie. Contraste de la hipótesis de raíz unitaria (método KPPS)\n",
    "<br><br>\n",
    "- **Sección 4.** División simple en conjuntos de entrenamiento y validación\n",
    "<br><br>\n",
    "- **Secciones 5, 6, 7, 8 y 9.** Predicciones\n",
    "<br><br>\n",
    "- **Sección 10.** Tabla con los valores MAPE de las predicciones\n",
    "\n",
    "## <a name=\"1.3\"></a> 1.3 - Métodos de predicción utilizados\n",
    "\n",
    "**En Python:**\n",
    "- I - Holt-Winters con estacionalidad\n",
    "\n",
    "- II - Prophet\n",
    "\n",
    "\n",
    "**En R:**\n",
    "\n",
    "\n",
    "- III - Métodos sencillos en R (media, naïve, etc)\n",
    "- IV - Regresión armónica dinámica con series de Fourier\n",
    "- V - TBATS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"1.4\"></a> 1.4 - Paquetes necesarios en Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()\n",
    "# import pandas_profiling\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import statsmodels.api as sm\n",
    "import xgboost as xgb\n",
    "from fbprophet import Prophet\n",
    "from fbprophet.diagnostics import cross_validation\n",
    "\n",
    "# graphic visualization packages\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Patch\n",
    "import seaborn as sns\n",
    "import chart_studio.plotly as py\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "import plotly.express as px\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name=\"1.5\"></a> 1.5 - Paquetes necesarios en R\n",
    "\n",
    "```r\n",
    "# Packages needed for the R environment:\n",
    "library(tidyverse)\n",
    "library(ggfortify)\n",
    "library(lubridate)\n",
    "library(forecast)\n",
    "library(tseries)\n",
    "library(urca)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name=\"2\"></a> 2 - Ingestión de datos y limpieza del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read the data into a pandas DataFrame\n",
    "pjme = pd.read_csv(\"data/PJME_hourly.csv\", parse_dates=[0], encoding='UTF-8')\n",
    "pjme.Datetime = pd.to_datetime(pjme.Datetime)\n",
    "pjme.rename(columns={'PJME_MW':'demand_in_MW'}, inplace=True)\n",
    "\n",
    "# Drop possible duplicate values and order the data by date and time\n",
    "pjme.drop_duplicates(subset='Datetime', keep='last', inplace=True)\n",
    "pjme.sort_values(by=['Datetime'], axis=0, ascending=True, inplace=True)\n",
    "# Reindex after the dataframe has been sorted\n",
    "pjme.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "# Now, let's check if the dataframe have some missing measurments\n",
    "pjme = pjme.set_index('Datetime')\n",
    "print(f'Datetime freq is set to: {pjme.index.freq}, i.e. the dataset is not continous') # Datatime index frequency is set to None\n",
    "                                                     # i.e. the time series is not cointinous\n",
    "\n",
    "# We can measure the number of missing measurements\n",
    "date_range = pd.date_range(start=min(pjme.index), \n",
    "                           end=max(pjme.index), \n",
    "                           freq='H')\n",
    "print(f'The number of missing measurements in the set is: {(len(date_range)-len(pjme))}:')\n",
    "\n",
    "# This will append the previously missing datetimes, and create null values in our target variable\n",
    "pjme = pjme.reindex(date_range)\n",
    "# Fill in the blanks with values that lie on a linear curve between existing data points\n",
    "pjme['demand_in_MW'].interpolate(method='linear', inplace=True)\n",
    "# Now the dataset is continously populated\n",
    "print(f'The pjme.index.freq is now: {pjme.index.freq}, indicating that we no longer have missing instances')\n",
    "# Lets copy the Datetime index again into a columns, we are going to need for further calculations\n",
    "pjme['Datetime'] = pjme.index\n",
    "pjme.Datetime = pd.to_datetime(pjme.Datetime)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "# Display the first and last rows\n",
    "display(pjme.head())\n",
    "display(pjme.tail())\n",
    "\n",
    "print(f\"Length of the dataset: {len(pjme):,}\\n\\n\\\n",
    "Number of N/A values:\\n{pd.isna(pjme).sum()}\\n\\\n",
    "Number of missing values:\\n{pd.isnull(pjme).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 - Análisis exploratorio de los datos (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create some seasonal features\n",
    "def create_features(df, label=None):\n",
    "    \"\"\"\n",
    "    Function to create several time-range features, day of the week, day of the year, day of month, \n",
    "    \"\"\"\n",
    "    \n",
    "    df['date'] = df.Datetime\n",
    "    df['hour'] = df['date'].dt.hour\n",
    "    df['dayofweek'] = df['date'].dt.dayofweek\n",
    "    df['quarter'] = df['date'].dt.quarter\n",
    "    df['month'] = df['date'].dt.month\n",
    "    df['year'] = df['date'].dt.year\n",
    "    df['dayofyear'] = df['date'].dt.dayofyear\n",
    "    df['dayofmonth'] = df['date'].dt.day\n",
    "    df['weekofyear'] = df['date'].dt.weekofyear   \n",
    "    # Create the season feature\n",
    "    conditions = [\n",
    "            (df['dayofyear'] < 79),\n",
    "            (df['dayofyear'] >= 356),\n",
    "            (df['dayofyear'] >= 79)  & (df['dayofyear'] < 172),\n",
    "            (df['dayofyear'] >= 172) & (df['dayofyear'] < 266),\n",
    "            (df['dayofyear'] >= 266  & (df['dayofyear'] < 356))]\n",
    "    choices = [0, 0, 1,2,3]\n",
    "    df['season'] = np.select(conditions, choices, default=0)\n",
    "        \n",
    "    X = df[['date','hour','dayofweek','month', 'quarter', 'season',\n",
    "           'dayofyear', 'dayofmonth', 'weekofyear', 'year']]\n",
    "    if label:\n",
    "        y = df[label]\n",
    "        return pd.concat([X, y], axis=1)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_months_days_to_string(df):\n",
    "    \"\"\"\n",
    "    Simple function to rename month and days of the week, from numeric values to chars.\n",
    "    \"\"\"\n",
    "\n",
    "    df['dayofweek_string'] = df['dayofweek'].map({0:'Monday', 1:'Tuesday', 2:'Wednesday', 3:'Thursday',\n",
    "                                                  4: 'Friday', 5:'Saturday', 6:'Sunday'})\n",
    "    df['season_string']    = df['season'].map({0:'winter', 1:'spring', 2:'summer', 3:'fall'})\n",
    "    \n",
    "    return df[['date','hour','dayofweek','dayofweek_string', 'month', 'quarter', 'season',\n",
    "               'season_string', 'dayofyear', 'dayofmonth', 'weekofyear', 'year', 'demand_in_MW']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I use the features_target dataframes for the EDA\n",
    "features_target = create_features(pjme, 'demand_in_MW')\n",
    "features_target = map_months_days_to_string(features_target)\n",
    "\n",
    "# I just mapped numerical categories to strings of the actual days of the week and seasons\n",
    "# for a better interpretability of the group by aggregation computations in the visual EDA section\n",
    "display(features_target.head())\n",
    "display(features_target.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure(layout=go.Layout(title=go.layout.Title(\n",
    "    text=f'Power Demand (MW) over time [{min(pjme.year)} - {max(pjme.year)}]')))\n",
    "fig.add_trace(go.Scatter(x=pjme.date, y=pjme.demand_in_MW, name='Demand in MW', line_color='chocolate'))\n",
    "fig.update_traces(line=dict(width=0.5))\n",
    "fig.update_xaxes(title_text='Time')\n",
    "fig.update_yaxes(title_text='Power Consumption (MW)')\n",
    "fig.update_layout(showlegend=True, width=1000)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the correlation matrix\n",
    "corr = features_target[['demand_in_MW', 'hour', 'dayofweek', 'month', 'season']].corr()\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.zeros_like(corr, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "# Generate a custom diverging colormap\n",
    "colormap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(corr, mask=mask, cmap=colormap, center=0, vmin=-1.0, vmax=1.0,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": 1}, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver, como la hora del día tiene una correlación positiva. Por lo que en principio el consumo subirá en horas más entrado el día. \n",
    "A su vez también podemos ver una ligera correlación negativa  del consumo eléctrico con el día de la semana. Teniendo en cuenta que hemos codificado los fines de semana como los días 5 y 6 de la semana, nos lleva a esperar una menor demanda durante el fin de semana."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 - En busca de patrones temporales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's aggregate the data by hour of the day during the week\n",
    "features_target_aggregated = features_target.groupby(['hour', 'dayofweek_string'], as_index=False).agg({'demand_in_MW':'median'})\n",
    "\n",
    "#plot\n",
    "fig = px.line(features_target_aggregated, x='hour', y='demand_in_MW', \n",
    "              color='dayofweek_string',\n",
    "              title='Median Hourly Power Demand per Weekday')\n",
    "fig.update_traces(mode='lines+markers')\n",
    "fig.update_layout(xaxis_title='Hour',\n",
    "                  yaxis_title='Energy Demand [MW]')\n",
    "fig.for_each_trace(\n",
    "    lambda trace: trace.update(name=trace.name.replace(\"dayofweek_string=\", \"\")),\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos observar como los fines de semana la demanda de electricidad es alrededor de un 10% menor que entre semana. A priori sería entendible plantear la hipótesis del consumo eléctrico durante la semana con el consumo que generan los puestos de trabajo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's aggregate by hour of the day during different seasons\n",
    "features_target_aggregated_by_season = features_target.groupby(['hour', 'season_string'], as_index=False)\\\n",
    "                                        .agg({'demand_in_MW':'median'})\n",
    "\n",
    "# plot\n",
    "color_map_season = {'winter':'mediumpurple', 'spring': 'forestgreen', 'summer': 'gold', 'fall':'chocolate'}\n",
    "fig = px.line(features_target_aggregated_by_season, x='hour', y='demand_in_MW', \n",
    "              color='season_string', title='Median Hourly Power Demand per Season', color_discrete_map=color_map_season)\n",
    "fig.update_traces(mode='lines+markers')\n",
    "fig.update_layout(xaxis_title='Hour',\n",
    "                  yaxis_title='Energy Demand [MW]')\n",
    "fig.for_each_trace(\n",
    "    lambda trace: trace.update(name=trace.name.replace(\"season_string=\", \"\")),\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aunque el coeficiente de correlación de Pearson era cercano a cero. Era esperado encontrar una estacionalidad alta del consumo eléctrico. Vemos como la demanda está muy estrechamente ligado al verano y en menor medida al invierno."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 - Descomposición de la Serie Temporal\n",
    "\n",
    "Uno de los aspectos más interesantes de las series temporales, es el poder descomponer la serie en diferentes componentes. Una componente de soporte principal de la tendencia, una componente periódica, que es la que caracteriza los ciclos estacionales.\n",
    "\n",
    "Como en este ejemplo la componente estacional es constante, como hemos podido ver en la Figura 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "# NOTE, that seasonal_decompose needs to have the dataframe with a Datetime object as index\n",
    "series = features_target[['demand_in_MW']]\n",
    "frequency = 24*365\n",
    "\n",
    "# decomposing the time-series, with the frequency being 24 hours per 365 days\n",
    "decomposed = seasonal_decompose(series, model='additive', freq=frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decompositions(decompositions, titles, line_widths):\n",
    "    \" Plotting the different elements constituting the time-series\"\n",
    "    \n",
    "    for data, title, lw in zip(decompositions, titles, line_widths):\n",
    "        # draw a line plot of the data with plotly express\n",
    "        fig = px.line(data, y='demand_in_MW',\n",
    "                      title=title, height=300)\n",
    "        # adjust line width\n",
    "        fig.update_traces(line=dict(width=lw), line_color='chocolate')\n",
    "        \n",
    "        fig.update_layout(xaxis=dict(showticklabels=False, linewidth=1),\n",
    "                          yaxis=dict(title=''), \n",
    "                          margin=go.layout.Margin(l=40, r=40, b=0, t=40, pad=0),)\n",
    "        if title=='Trend':\n",
    "            fig.update_yaxes(range=[10000,65000])\n",
    "        \n",
    "        fig.update_layout(xaxis_title=\"Years\")\n",
    "        fig.show()\n",
    "\n",
    "# calling the function \n",
    "plot_decompositions(decompositions=[decomposed.trend, decomposed.seasonal, decomposed.resid],\n",
    "                    titles=['Trend', 'Seasonality', 'Residuals'],\n",
    "                    line_widths=[2, 0.05, 0.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La razón del modelo aditivo, es porque se trata de un modelo lineal, en el que la componente de tendencia (***trend***) tiende a ser constante, la componente estacional (***seasonal***) tiene una periodicidad (amplitud y frecuencia constantes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name=\"3\"></a> 3 - Comprobación de la estacionariedad de la serie (hipótesis de raíz unitaria) - R\n",
    "\n",
    "A priori, debido a que la tendencia de la serie temporal es visualmente muy \"plana\", podríamos pensar que se trata de una serie estacionaria. Pero aún así, vamos a realizar una comprobación más formal de ello utilizando el método KPSS para comprobar la hipótesis de la **raíz unitaria**.\n",
    "\n",
    "Hay varios métodos a disposicón para comprobar la estacionalidad de una serie temporal:\n",
    "-Métodos cualitativos visuales (gráfica de los datos, gráficas de autocorrelación, estadísticos móviles, etc.)\n",
    "-Métodos cuantitativos como el Alternative Dickey-Fuller Test (ADF) o el método KPSS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primer punto donde introducimos código en R. Vamos a proceder a evaluar la estacionalidad de los datos, utilizando el método KPSS incluido en el paquete forecast() de R.\n",
    "\n",
    "```r\n",
    "# First, we import the data into the R workspace\n",
    "pjme = read.csv(\"data/PJME_hourly.csv\")\n",
    "pjme[1] <- lapply(pjme[1], as.character)\n",
    "pjme[,1]  <-  parse_date_time(pjme[,1], \"ymd HMS\")\n",
    "pjme <- distinct(pjme,Datetime, .keep_all = TRUE)\n",
    "pjme <- arrange(pjme,Datetime)\n",
    "pjme <- pjme[order(pjme$Datetime),]\n",
    "\n",
    "# Train/Test split. Train: From 1st Jan 2016 to 30th Nov 2017. Test: From 1st Dec 2017 onwards\n",
    "train <- pjme %>% \n",
    "  filter(Datetime < ymd_hms(\"2017-12-01 00:00:00\")) %>% \n",
    "  filter(Datetime > ymd_hms(\"2015-12-31 23:00:00\"))\n",
    "test  <- pjme %>% \n",
    "  filter(Datetime > ymd_hms(\"2017-11-30 23:00:00\"))\n",
    "\n",
    "# Create the msts() object. Which is basically a Time Series object that accepts multiple seasonality\n",
    "train_ts     <- msts(train$PJME_MW, start = c(2016,0), ts.frequency = 24*365.25, seasonal.periods = c(24,24*365.25))\n",
    "# Since we analyzed the data and reach to the conclusion that weekly seasonality is not as strong as daily and yearly,\n",
    "# I did not included it in the seasonal.periods vector, thus c(24, 24*365.25).\n",
    "             \n",
    "\n",
    "             \n",
    "train_ts %>% diff() %>% ur.kpss() %>% summary()\n",
    "\n",
    "ndiffs(train_ts) # We should take a first difference for the data\n",
    "\n",
    "train_ts %>% diff(lag=24) %>% ndiffs() # No seasonal difference.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pasamos a realizar las pruebas de raíz unitaria con la función KPSS del paquete urca."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```r\n",
    "# ====================\n",
    "# Stationarity check\n",
    "# ====================\n",
    ">>> train_ts %>% ur.kpss() %>% summary() # Non-differenced data\n",
    "####################### \n",
    "# KPSS Unit Root Test # \n",
    "####################### \n",
    "\n",
    "Test is of type: mu with 14 lags. \n",
    "\n",
    "Value of test-statistic is: 2.2087 \n",
    "\n",
    "Critical value for a significance level of: \n",
    "                10pct  5pct 2.5pct  1pct\n",
    "critical values 0.347 0.463  0.574 0.739\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**No** podemos descartar la hipótesis nula de no haber una raíz unitaria en la serie temporal, por lo que vamos a diferenciar la serie una vez"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```r\n",
    ">>> train_ts %>% diff() %>% ur.kpss() %>% summary()\n",
    "####################### \n",
    "# KPSS Unit Root Test # \n",
    "####################### \n",
    "\n",
    "Test is of type: mu with 14 lags. \n",
    "\n",
    "Value of test-statistic is: 8e-04 \n",
    "\n",
    "Critical value for a significance level of: \n",
    "                10pct  5pct 2.5pct  1pct\n",
    "critical values 0.347 0.463  0.574 0.739\n",
    "\n",
    ">>> ndiffs(train_ts) # We should take ONE first difference for the data\n",
    "1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tras realizar una diferenciación, los datos son **estacionarios**. Este es un punto muy importante del análisis, ya que el parámetro **d** en cualquier modelo de la familia ARIMA se decide con esta condición. Como he diferenciado una vez hasta hacer estacionaria la serie d=1.\n",
    "\n",
    "Hacemos lo mimso con la diferencia estaciona y vemos que el resultado es nulo, por lo que no sería necesario diferenciar para hacer estacionaria la serie temporal. D=0. Es importante también tener en cuenta que el valor de D se puede ver a simple vista ya que la estacionalidad diaria (lag=24) está muy marcada y se ve que es fácilmente predecible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```r\n",
    ">>> train_ts %>% diff(lag=24) %>% ndiffs() # No seasonal difference needed\n",
    "0\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "\n",
    "# Let's plot the hourle autocorrelation.\n",
    "fig, ax = plt.subplots(2, figsize=(12,6))\n",
    "ax[0] = plot_acf(features_target.demand_in_MW.diff().dropna(), lags=168, ax=ax[0])\n",
    "ax[1] = plot_pacf(features_target.demand_in_MW.diff().dropna(), lags=168, ax=ax[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos en la gráfica de autocorrelación como la estacionalidad diaría es muy importante, ya que cada 24 horas tenemos un pico de autocorrelación. Se puede concluir que no hay una gran estacionalidad semanal. Son conclusiones a las que también he llegado utilizando la función **forecast::mstl()** de R."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name=\"4\"></a> 4 - División en conjunto de entrenamiento y validación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para el proyecto, vamos a reducir los datos a un periodo de dos años y un trimestre. Debido a la frecuencia horaria de este data-set y de la estacionalidad diaria y anual, un periodo de dos años puede contener los suficientes datos como para que nuestros modelos realicen sus estimaciones, además, reducimos el coste computacional de los modelos de predicción."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En principio y por temas de tiempo de ejecución de los distintos modelos. He obtado por una evaluación simple, con una división de los datos en entrenamiento y test del 66% 34% de la longitud de la serie. Ver **Anexo** para ver más información acerca de la **validación cruzada** de series temporales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ts_train_test_split(df, init_date, end_date, cutoff_date):\n",
    "    \"\"\"\n",
    "    Function to return a simple train/test split of the given a univariate time-series, within dates provided by the user.\n",
    "    The date should be given as a string in ISO format YYYY-MM-DD.\n",
    "    \"\"\"\n",
    "    \n",
    "    start = init_date\n",
    "    end   = end_date\n",
    "    print(f'The first date time point is: {start}')\n",
    "    print(f'The last date time point is: {max(df.index)}')\n",
    "    \n",
    "    range_df = len(pd.date_range(start=start,end=end, freq='Y'))\n",
    "    print(f\"The difference is: {range_df} years\")\n",
    "    print(f'The training range has to be {range_df*0.66} years')\n",
    "        \n",
    "    CUTOFF_DATE = pd.to_datetime(cutoff_date)\n",
    "    train = df.loc[(df.index < CUTOFF_DATE) & (df.index > init_date)].copy(deep=True)\n",
    "    test = df.loc[df.index >= CUTOFF_DATE].copy(deep=True)\n",
    "\n",
    "    return train, test\n",
    "\n",
    "train, test = ts_train_test_split(features_target, '2016-01-01 00:00:00', max(features_target.index), '2017-12-01')\n",
    "display(train.tail())\n",
    "display(test.head())\n",
    "\n",
    "y_train = train['demand_in_MW']\n",
    "y_test  = test['demand_in_MW']\n",
    "display(y_test)\n",
    "y_train.to_csv(\"data/pmje_train.csv\",index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Train/Test\n",
    "fig = go.Figure(layout=go.Layout(title=go.layout.Title(\n",
    "    text=f'Power Demand (MW) over time [{min(train.year)} - {max(test.year)}]')))\n",
    "fig.add_trace(go.Scatter(x=train.date, y=train.demand_in_MW, name='Training', line_color='chocolate'))\n",
    "fig.add_trace(go.Scatter(x=test.date, y=test.demand_in_MW, name='Test', line_color='green'))\n",
    "fig.update_traces(line=dict(width=0.5))\n",
    "fig.update_xaxes(title_text='Time')\n",
    "fig.update_yaxes(title_text='Power Consumption (MW)')\n",
    "fig.update_layout(showlegend=True, width=1000)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name=\"5\"></a> 5 - Método I: Holt-Winters con triple suavizamiento exponencial\n",
    "\n",
    "El algoritmo de **Holt-Winters** es un algoritmo basado en medias móviles muy utilizado para la predicción de series temporales. Debido a su simplicidad e interpretabilidad, vamos a utilizarlo como algoritmo base con el que después comparar los siguientes.\n",
    "\n",
    "La mayor limitación de este algoritmo es la capacidad de tener en cuenta sólo una estacionalidad. Nos decantamos por tomar el periodo anual, seasonal_periods=24\\*365"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "# exponential smoothing only takes into consideration patterns in the target variable\n",
    "# so we discard the other features\n",
    "\n",
    "# fit & predict\n",
    "holt_winter = sm.tsa.ExponentialSmoothing(y_train, seasonal='add', seasonal_periods=24*365).fit()\n",
    "y_hat_holt_winter = holt_winter.forecast(len(y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create figure\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=y_test.index, y=y_test,\n",
    "                         mode='lines',\n",
    "                         name='Test - Ground Truth'))\n",
    "fig.add_trace(go.Scatter(x=y_hat_holt_winter.index, y=y_hat_holt_winter,\n",
    "                         mode='lines', \n",
    "                         name='Test - Prediction'))\n",
    "\n",
    "# adjust layout\n",
    "fig.update_traces(line=dict(width=0.5))\n",
    "fig.update_layout(title='Holt-Winter Forecast of Hourly Energy Demand',\n",
    "                  xaxis_title='Date & Time',\n",
    "                  yaxis_title='Energy Demand [MW]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A primera vista el modelo parece obtener una buena aproximación de las series temporales. Pero vamos a evaluarlo de calculando el Mean Absolute Percentage Error (MAPE) entre la parte de test y la parte predicha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mape(y_true, y_pred):\n",
    "    \"\"\" Mean Absolute Percentage Error \"\"\"\n",
    "    \n",
    "    # convert to numpy arrays\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    \n",
    "    # take the percentage error\n",
    "    pe = (y_true - y_pred) / y_true\n",
    "    \n",
    "    # take the absolute values\n",
    "    ape = np.abs(pe)\n",
    "    \n",
    "    # quantify the performance in a single number\n",
    "    mape = np.mean(ape)\n",
    "    \n",
    "    return f'{mape*100:.2f}%'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mape_hw = mape(y_true=y_test, y_pred=y_hat_holt_winter)\n",
    "print(f'Our Holt-Winter model has a mean average percentage error (MAPE) of: {mape_hw}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se encuentran las gráficas de los resultados en una ventana de tiempo menor (semanal). Para ver como ha reaccionado el modelo a las estacionalidades de menor orden de los datos (estacionalidad semanal)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interval length\n",
    "interval = 24 * 7\n",
    "\n",
    "# intermediary variables for readability\n",
    "x_true, y_true = y_test.iloc[:interval].index, y_test.iloc[:interval]\n",
    "x_pred, y_pred = y_hat_holt_winter.iloc[:interval].index, y_hat_holt_winter.iloc[:interval]\n",
    "\n",
    "# create figure\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=x_true, y=y_true,\n",
    "                         mode='lines',\n",
    "                         name='Test - Ground Truth'))\n",
    "fig.add_trace(go.Scatter(x=x_pred, y=y_pred,\n",
    "                         mode='lines', \n",
    "                         name='Test - Prediction'))\n",
    "\n",
    "# adjust layout\n",
    "fig.update_traces(line=dict(width=0.9))\n",
    "fig.update_layout(title=f'Holt-Winter Intra-Day Forecast of First {interval} Hours of Energy Demand',\n",
    "                  xaxis_title='Date & Time',\n",
    "                  yaxis_title='Energy Demand [MW]')\n",
    "fig.show()\n",
    "\n",
    "# quantify accuracy\n",
    "print(f'MAPE for interval of the first {interval} hours: {mape(y_true, y_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La misma ventana de tiempo pero para la última semana de predicciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interval length\n",
    "interval = -24 * 7\n",
    "\n",
    "# intermediary variables for readability\n",
    "x_true, y_true = y_test.iloc[interval:].index, y_test.iloc[interval:]\n",
    "x_pred, y_pred = y_hat_holt_winter.iloc[interval:].index, y_hat_holt_winter.iloc[interval:]\n",
    "\n",
    "# create figure\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=x_true, y=y_true,\n",
    "                         mode='lines',\n",
    "                         name='Test - Ground Truth'))\n",
    "fig.add_trace(go.Scatter(x=x_pred, y=y_pred,\n",
    "                         mode='lines', \n",
    "                         name='Test - Prediction'))\n",
    "\n",
    "# adjust layout\n",
    "fig.update_traces(line=dict(width=0.9))\n",
    "fig.update_layout(title=f'Holt-Winter Intra-Day Forecast of Last {abs(interval)} Hours of Energy Demand',\n",
    "                  xaxis_title='Date & Time (yyyy/mm/dd hh:MM)',\n",
    "                  yaxis_title='Energy Demand [MW]')\n",
    "fig.show()\n",
    "\n",
    "# quantify accuracy\n",
    "print(f'MAPE for interval of the last {abs(interval)} hours: {mape(y_true, y_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 - El problema de la estacionalidad múltiple\n",
    "\n",
    "El método de Holt-Winters con estacionalidad no está pensado para capturar estacionalidades múltiples como la del set de datos escogidos. Hay otros modelos más avanzados (y también más costosos computacionalmente), como los modelos BATS, TBATS, o modelos de suavizado exponencial con componentes externas en formas de transformadas de Fourier, que deberían de dar mejores resultados.\n",
    "\n",
    "He intentando utilizar una implementación del modelo TBATS codificado en Python para estacionalidad múltiple, pero no he logrado hacer converger el modelo, por lo que no he podido obtener los datos para la comparación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name=\"6\"></a> 6 - Método II: Predicción utilizando la librería **Prophet** de Facebook\n",
    "\n",
    "En el siguiente apartado vamos a utilizar la librería de código libre creada por Facebook, Prophet, que es un modelo aditivo/multiplicativo, muy flexible y avanzado, que es capaz de lidiar con estacionalidades múltiples, eventos y fiestas nacionales (como Navidad, Año Nuevo, etc), así como la posibilidad de introducir eventos de fecha variable como puedan ser, finales de un evento deportivo de máximo interés, Semana Santa, Black Friday, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fbprophet import Prophet\n",
    "from fbprophet.diagnostics import cross_validation\n",
    "\n",
    "# format data for prophet model using 'ds' and 'y' as per the Python API\n",
    "train_prophet = y_train.reset_index().rename(columns={'index':'ds', 'demand_in_MW':'y'})\n",
    "\n",
    "test_prophet = y_test.reset_index().rename(columns={'index':'ds', 'demand_in_MW':'y'})\n",
    "\n",
    "display(train_prophet.tail())\n",
    "display(test_prophet.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conditions for the variable seasonality of the data\n",
    "def is_spring(ds):\n",
    "    date = pd.to_datetime(ds)\n",
    "    return (date.dayofyear >= 79) & (date.dayofyear < 172)\n",
    "\n",
    "def is_summer(ds):\n",
    "    date = pd.to_datetime(ds)\n",
    "    return (date.dayofyear >= 172) & (date.dayofyear < 266)\n",
    "\n",
    "def is_autumn(ds):\n",
    "    date = pd.to_datetime(ds)\n",
    "    return (date.dayofyear >= 266) & (date.dayofyear < 356)\n",
    "\n",
    "def is_winter(ds):\n",
    "    date = pd.to_datetime(ds)\n",
    "    return (date.dayofyear < 79) | (date.dayofyear >= 356)\n",
    "\n",
    "def is_weekend(ds):\n",
    "    date = pd.to_datetime(ds)\n",
    "    return date.day_name in ('Saturday', 'Sunday')\n",
    "\n",
    "# adding to train set\n",
    "train_prophet['is_spring'] = train_prophet['ds'].apply(is_spring)\n",
    "train_prophet['is_summer'] = train_prophet['ds'].apply(is_summer)\n",
    "train_prophet['is_autumn'] = train_prophet['ds'].apply(is_autumn)\n",
    "train_prophet['is_winter'] = train_prophet['ds'].apply(is_winter)\n",
    "train_prophet['is_weekend'] = train_prophet['ds'].apply(is_weekend)\n",
    "train_prophet['is_weekday'] = ~train_prophet['ds'].apply(is_weekend)\n",
    "\n",
    "# adding to test set\n",
    "test_prophet['is_spring'] = test_prophet['ds'].apply(is_spring)\n",
    "test_prophet['is_summer'] = test_prophet['ds'].apply(is_summer)\n",
    "test_prophet['is_autumn'] = test_prophet['ds'].apply(is_autumn)\n",
    "test_prophet['is_winter'] = test_prophet['ds'].apply(is_winter)\n",
    "test_prophet['is_weekend'] = test_prophet['ds'].apply(is_weekend)\n",
    "test_prophet['is_weekday'] = ~test_prophet['ds'].apply(is_weekend)\n",
    "\n",
    "display(train_prophet.tail())\n",
    "display(test_prophet.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiating the Prophet class with user defined settings\n",
    "prophet = Prophet(daily_seasonality=False, weekly_seasonality=False,\n",
    "                  yearly_seasonality=False)\n",
    "\n",
    "# custom seasonalities to account for conditional variance \n",
    "# (more extreme trends in extreme seasons)\n",
    "prophet.add_seasonality(name='yearly', period=365.25, fourier_order=10)\n",
    "prophet.add_seasonality(name='weekly_spring', \n",
    "                        period=7,\n",
    "                        fourier_order=5, \n",
    "                        condition_name='is_spring')\n",
    "prophet.add_seasonality(name='weekly_summer', \n",
    "                        period=7,\n",
    "                        fourier_order=5, \n",
    "                        condition_name='is_summer')\n",
    "prophet.add_seasonality(name='weekly_autumn', \n",
    "                        period=7,\n",
    "                        fourier_order=5, \n",
    "                        condition_name='is_autumn')\n",
    "prophet.add_seasonality(name='weekly_winter', \n",
    "                        period=7,\n",
    "                        fourier_order=5, \n",
    "                        condition_name='is_winter')\n",
    "prophet.add_seasonality(name='daily_spring',  \n",
    "                        period=1,\n",
    "                        fourier_order=5, \n",
    "                        condition_name='is_spring')\n",
    "prophet.add_seasonality(name='daily_summer',  \n",
    "                        period=1,\n",
    "                        fourier_order=5, \n",
    "                        condition_name='is_summer')\n",
    "prophet.add_seasonality(name='daily_autumn',  \n",
    "                        period=1,\n",
    "                        fourier_order=5, \n",
    "                        condition_name='is_autumn')\n",
    "prophet.add_seasonality(name='daily_winter',  \n",
    "                        period=1,\n",
    "                        fourier_order=5, \n",
    "                        condition_name='is_winter')\n",
    "prophet.add_seasonality(name='daily_weekend',  \n",
    "                        period=1,\n",
    "                        fourier_order=5, \n",
    "                        condition_name='is_weekend')\n",
    "prophet.add_seasonality(name='daily_weekday',  \n",
    "                        period=1,\n",
    "                        fourier_order=5, \n",
    "                        condition_name='is_weekday')\n",
    "\n",
    "# fitting the model\n",
    "prophet.fit(train_prophet);\n",
    "\n",
    "# part of the dataframe on which we want to make predictions\n",
    "future = test_prophet.drop(['y'], axis=1)\n",
    "\n",
    "# predicting values\n",
    "forecast = prophet.predict(future)\n",
    "\n",
    "# see https://github.com/facebook/prophet/issues/999 for the matplotlib_converts()\n",
    "pd.plotting.register_matplotlib_converters()\n",
    "\n",
    "# plotting the seasonality components found\n",
    "prophet.plot_components(forecast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create figure\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=test_prophet.ds, y=test_prophet.y,\n",
    "                         mode='lines',\n",
    "                         name='Test - Ground Truth'))\n",
    "fig.add_trace(go.Scatter(x=forecast.ds, y=forecast.yhat,\n",
    "                         mode='lines', \n",
    "                         name='Test - Prediction'))\n",
    "\n",
    "# adjust layout\n",
    "fig.update_traces(line=dict(width=0.5))\n",
    "fig.update_layout(title='Prophet Forecast of Hourly Energy Demand',\n",
    "                  xaxis_title='Date & Time (yyyy/mm/dd hh:MM)',\n",
    "                  yaxis_title='Energy Demand [MW]')\n",
    "fig.show()\n",
    "\n",
    "# quantify accuracy\n",
    "print(f'MAPE for Prophet\\'s predictions: {mape(test_prophet.y, forecast.yhat)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interval length\n",
    "interval = 24 * 7\n",
    "\n",
    "# intermediary variables for readability\n",
    "x_true, y_true = test_prophet.iloc[:interval].ds, test_prophet.iloc[:interval].y\n",
    "x_pred, y_pred = forecast.iloc[:interval].ds, forecast.iloc[:interval].yhat\n",
    "\n",
    "# create figure\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=x_true, y=y_true,\n",
    "                         mode='lines',\n",
    "                         name='Test - Ground Truth'))\n",
    "fig.add_trace(go.Scatter(x=x_pred, y=y_pred,\n",
    "                         mode='lines', \n",
    "                         name='Test - Prediction'))\n",
    "\n",
    "# adjust layout\n",
    "fig.update_traces(line=dict(width=0.9))\n",
    "fig.update_layout(title=f'Prophet Intra-Day Forecast of First {interval} Hours of Energy Demand',\n",
    "                  xaxis_title='Date & Time (yyyy/mm/dd hh:MM)',\n",
    "                  yaxis_title='Energy Demand [MW]')\n",
    "fig.show()\n",
    "\n",
    "# quantify accuracy\n",
    "print(f'MAPE for interval of the first {interval} hours: {mape(y_true, y_pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interval length\n",
    "interval = -24 * 7\n",
    "\n",
    "# intermediary variables for readability\n",
    "x_true, y_true = test_prophet.iloc[interval:].ds, test_prophet.iloc[interval:].y\n",
    "x_pred, y_pred = forecast.iloc[interval:].ds, forecast.iloc[interval:].yhat\n",
    "\n",
    "# create figure\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=x_true, y=y_true,\n",
    "                         mode='lines',\n",
    "                         name='Test - Ground Truth'))\n",
    "fig.add_trace(go.Scatter(x=x_pred, y=y_pred,\n",
    "                         mode='lines', \n",
    "                         name='Test - Prediction'))\n",
    "\n",
    "# adjust layout\n",
    "fig.update_traces(line=dict(width=0.9))\n",
    "fig.update_layout(title=f'Prophet Intra-Day Forecast of Last {abs(interval)} Hours of Energy Demand',\n",
    "                  xaxis_title='Date & Time (yyyy/mm/dd hh:MM)',\n",
    "                  yaxis_title='Energy Demand [MW]')\n",
    "fig.show()\n",
    "\n",
    "# quantify accuracy\n",
    "print(f'MAPE for interval of the last {abs(interval)} hours: {mape(y_true, y_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name=\"7\"></a> 7 - Método III: Métodos de predicción simples. Implementación en R\n",
    "    \n",
    "A la hora de realiar las predicciones en R me he decantado por utilizar el paquete **forecast** de Rob Hyndman. La documentación está muy detallada, y además es posible encontrar numerosos ejemplos en su libro *Forecasting: Principles and Practice*.\n",
    "\n",
    "Como vimos en el apartado de de las pruebas de raíz unitaria en R. El objeto base del paquete forecast es el objeto ts de base::R o msts de forecast. El primero es un objeto time-series con frecuencia regular y única. Mientras que msts también es un objeto de frecuencia regular, pero permite pasarle un vector con las frecuencias más importantes (diarias, semanales, anuales, etc) en función de la serie temporal analizada.\n",
    "\n",
    "En los cálculos realizados en R, he seguido un proceso incremental. Primero he analizado la serie con los métodos más sencillos (aunque bastante robustos y eficientes) y he ido aumentando la complejidad de los modelos utilizados (Regresión armónica dinámica con series de Fourier o TBATS). \n",
    "\n",
    "En los siguientes puntos de la sección 7, mostraré los resultados obtenidos con estos métodos más sencillos.\n",
    "\n",
    "## 7.1 - Mean method\n",
    "\n",
    "Este método consiste simplemente en dar a todos los valores futuros, el valor medio de la serie.\n",
    "\n",
    "```r\n",
    ">>> pjme_mean    <- meanf(train_ts, h=24*245)\n",
    "\n",
    "# Plot\n",
    ">>> autoplot(train_ts) +\n",
    "  autolayer(test_ts, series=\"Test\") +\n",
    "  autolayer(pjme_mean, series=\"Mean\", PI=FALSE) +\n",
    "  ggtitle(\"Power Demand (MW) over time [2016 - 2018]\") +\n",
    "  xlab(\"Year\") + ylab(\"Demand in MW\") +\n",
    "  guides(colour=guide_legend(title=\"Forecast\"))\n",
    "\n",
    "# MAPE (%) computation\n",
    ">>> acc1 <- round(accuracy(pjme_mean, test_ts),2)[2,5]\n",
    ">>> sprintf(\"Mean method MAPE: %s %%\", acc1)\n",
    "```\n",
    "> **\"Mean method MAPE: 14.7 %\"**\n",
    "\n",
    "<img src=\"figures/triple_seasonality_dwy/mean_method.png\" align=\"center\"/>\n",
    "\n",
    "## 7.2 - Naïve method\n",
    "\n",
    "En este caso, todas las predicciones reciben el valor del último dato disponible.\n",
    "\n",
    "```r\n",
    ">>> pjme_naive   <- naive(train_ts, h=24*245)\n",
    "\n",
    "# Plot\n",
    ">>> autoplot(train_ts) +\n",
    "  autolayer(test_ts, series=\"Test\") +\n",
    "  autolayer(pjme_naive, series=\"Naïve\", PI=FALSE) +\n",
    "  ggtitle(\"Power Demand (MW) over time [2016 - 2018]\") +\n",
    "  xlab(\"Year\") + ylab(\"Demand in MW\") +\n",
    "  guides(colour=guide_legend(title=\"Forecast\"))\n",
    "\n",
    "# MAPE (%) computation\n",
    ">>> acc2 <- round(accuracy(pjme_naive, test_ts),2)[2,5]\n",
    ">>> sprintf(\"Naive method MAPE: %s %%\", acc2)\n",
    "```\n",
    "> **\"Naive method MAPE: 15.18 %\"**\n",
    "\n",
    "<img src=\"figures/triple_seasonality_dwy/naive_method.png\" align=\"center\"/>\n",
    "\n",
    "## 7.3 - Naïve with drift method\n",
    "\n",
    "En este método se permite aumentar o disminuir el valor de las predicciones en el tiempo. Es como si trazasemos una línea entre el primer punto histórico de la serie y el último y extrapolásemos con la misma pendiente a valores futuros.\n",
    "\n",
    "```r\n",
    ">>> pjme_drift   <- rwf(train_ts, h=24*245, drift=TRUE)\n",
    "\n",
    "# Plot\n",
    ">>> autoplot(train_ts) +\n",
    "  autolayer(test_ts, series=\"Test\") +\n",
    "  autolayer(pjme_drift, series=\"Drift\", PI=FALSE) +\n",
    "  ggtitle(\"Power Demand (MW) over time [2016 - 2018]\") +\n",
    "  xlab(\"Year\") + ylab(\"Demand in MW\") +\n",
    "  guides(colour=guide_legend(title=\"Forecast\"))\n",
    "\n",
    "# MAPE (%) computation\n",
    ">>> acc3 <- round(accuracy(pjme_drift, test_ts),2)[2,5]\n",
    ">>> sprintf(\"Naive method with drift MAPE: %s %%\", acc3)\n",
    "```\n",
    "> **\"Naive method with drift MAPE: 15.04 %\"**\n",
    "\n",
    "<img src=\"figures/triple_seasonality_dwy/naive_drift.png\" align=\"center\"/>\n",
    "\n",
    "## 7.4 - Seasonal Naïve method\n",
    "\n",
    "Los visualización de las gráficas dadas por este método parecen más complicadas de lo que en realidad son. Pero simplemente los puntos de las predicciones son los valores en estacionalidades anteriores. Es decir, si tenemos un valor y_t+1 = y_t en la estacionalidad pasada. Por ejemplo los puntos en Diciembre de 2019 tendrán el valor de puntos en Diciembre de 2018.\n",
    "\n",
    "```r\n",
    ">>> pjme_s_naive <- snaive(train_ts, h=24*245)\n",
    "\n",
    "# Plot\n",
    ">>> autoplot(train_ts) +\n",
    "  autolayer(test_ts, series=\"Test\") +\n",
    "  autolayer(pjme_s_naive, series=\"Seasonal naïve\", PI=FALSE, alpha=0.7) +\n",
    "  ggtitle(\"Power Demand (MW) over time [2016 - 2018]\") +\n",
    "  xlab(\"Year\") + ylab(\"Demand in MW\") +\n",
    "  guides(colour=guide_legend(title=\"Forecast\"))\n",
    "\n",
    "# MAPE (%) computation\n",
    ">>> acc4 <- round(accuracy(pjme_s_naive, test_ts),2)[2,5]\n",
    ">>> sprintf(\"Seasonal Naive method MAPE: %s %%\", acc4)\n",
    "```\n",
    "> **\"Seasonal naïve method MAPE: 13.12 %\"**\n",
    "\n",
    "<img src=\"figures/triple_seasonality_dwy/seas_naive.png\" align=\"center\"/>\n",
    "<br>\n",
    "<img src=\"figures/triple_seasonality_dwy/seas_naive_zoom.png\" align=\"center\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name=\"8\"></a> 8 - Método IV: Regresión armónica dinámica (DHR) para estacionalidades múltiples - Implementado en R\n",
    "\n",
    "Cuando hay estacionalidades muy largas, una regresión armónica dinámica utilizando series de Fourier para modelizar los regresores externos es más indicado que métodos como ARIMA o ETS.\n",
    "En el dataset escogido para este trabajo, al tener datos por hora, estamos hablando de que cualquier estacionalidad presente va a ser muy alta, m=24 para estacionalidad diaria, m=168 para una estacionalidad semanal, m=8766 para estacionalidades anuales. La principal diferencia es que los métodos de la familia ARIMA o ETS están concebidos para estacionalidades más bajas (mensuales, m=12 o trimestrales m=4). La principal razón es de coste computacional, ya que estos métodos necesitan calcular m-1 parámetros, con el consiguiente coste computacional y de memoria.\n",
    "\n",
    "En los métodos de regresión armónica, la componente estacional se calcula añadiendo transformadas de Fourier en las distintas frecuencias de cada estacionalidad modelada. El movimiento periódico de corta duración de la serie se modeliza con un modelo ARMA (modelo autoregresivo de media móvil).\n",
    "\n",
    "Las principales ventajas que ofrece este método son:\n",
    "- Estacionalidad de cualquier duración (para diferentes estacionalidad, se añaden distintos términos de Fourier con distintas frecuencias).\n",
    "<br><br>\n",
    "- La suavidad del patrón estacional puede controlarse con K, el número de pares de cosenos y senos en cada Transformada de Fourier para cada frecuencia. Un valor más bajo de K implica un modelado más suave.\n",
    "<br><br>\n",
    "- El movimiento periódico a corto plazo se modela con un simple modelo ARMA.\n",
    "\n",
    "## 8.1 - Implementación en R\n",
    "\n",
    "Para implementar este método, me he decantado por utilizar la librería forecast() de Rob Hyndman. La forma de implementarlo es la siguiente:\n",
    "\n",
    "Primero utilizamos la función auto.arima().\n",
    "```r\n",
    "fit <- auto.arima(train_ts, seasonal=TRUE, lambda = 0, xreg = fourier(train_ts, K=c(12,2)))\n",
    "```\n",
    "xreg, es el parámetro más importante porque es el que se encarga de modelizar las estacionalidades y añadirlas al modelo ARMA.\n",
    "\n",
    "La componente ARMA del modelo lo selecciona de forma automática utilizando como criterio la minimización del **AICc** (bias corrected Akaike information criterion).\n",
    "\n",
    "```r\n",
    ">>> fit # Output resumen del modelo:\n",
    "Series: train_ts \n",
    "Regression with ARIMA(5,1,5) errors \n",
    "Box Cox transformation: lambda= 0 \n",
    "\n",
    "Coefficients:\n",
    "         ar1      ar2     ar3      ar4     ar5      ma1      ma2      ma3    ma4     ma5\n",
    "      1.6553  -1.2152  1.4254  -1.2693  0.3028  -0.6828  -0.0202  -0.8966  0.289  0.4407\n",
    "s.e.  0.0142   0.0229  0.0197   0.0202  0.0126   0.0134   0.0142   0.0070  0.013  0.0096\n",
    "      drift    S1-24    C1-24    S2-24    C2-24    S3-24   C3-24   S4-24    C4-24    S5-24\n",
    "      0e+00  -0.1268  -0.0817  -0.0644  -0.0042  -0.0041  0.0026  0.0044  -0.0036  -0.0012\n",
    "s.e.  1e-04   0.0042   0.0043   0.0007   0.0007   0.0004  0.0004  0.0003   0.0003   0.0002\n",
    "        C5-24   S6-24   C6-24  S7-24  C7-24  S8-24  C8-24   S9-24  C9-24  S10-24  C10-24\n",
    "      -0.0043  -1e-03  -7e-04  7e-04  7e-04      0  1e-04  -1e-04      0  -1e-04       0\n",
    "s.e.   0.0002   1e-04   1e-04  1e-04  1e-04      0  0e+00   0e+00      0   0e+00       0\n",
    "      S11-24  C11-24  C12-24   S1-168  C1-168  S2-168  C2-168  S1-8766  C1-8766  S2-8766\n",
    "           0       0       0  -0.0439  0.0174  0.0211  0.0233   0.0191  -0.0330   0.1163\n",
    "s.e.       0       0       0   0.0053  0.0053  0.0025  0.0026   0.3038   0.2747   0.1426\n",
    "      C2-8766  S3-8766  C3-8766  S4-8766  C4-8766\n",
    "       0.0976  -0.0064  -0.0053   0.0058   0.0081\n",
    "s.e.   0.1387   0.0936   0.0935   0.0695   0.0706\n",
    "\n",
    "sigma^2 estimated as 0.0002021:  log likelihood=47630.77\n",
    "AIC=-95167.55   AICc=-95167.28   BIC=-94804.29\n",
    "```\n",
    "Una vez tenemos el modelo, el siguiente paso es hacer una gráfica de las predicciones frente a los valores de test, y comprobar que los residuos tengan una apariencia de *ruido blanco*:\n",
    "\n",
    "```r\n",
    ">>> fit <- auto.arima(train_ts, seasonal=TRUE, lambda = 0, xreg = fourier(train_ts,                         K=c(12,2,4)))\n",
    "\n",
    "# Plot:\n",
    "fit %>% \n",
    "  forecast(xreg=fourier(train_ts, K=c(12,2,4), h=24*245), level = 99) %>% \n",
    "  autoplot(include=945*24, PI=FALSE) +\n",
    "  autolayer(test_ts, series=\"Test\", alpha=0.8) +\n",
    "  ylab(\"Demand in MW\") + xlab(\"Date\")\n",
    "\n",
    "checkresiduals(fit)\n",
    "```\n",
    "> **\"Dynamic Regression with Fourier terms MAPE: 1.04 %\"**\n",
    "\n",
    "<img src=\"figures/triple_seasonality_dwy/dynamic_reg.png\" align=\"center\"/>\n",
    "<br>\n",
    "<img src=\"figures/triple_seasonality_dwy/dynamic_reg_zoom.png\" align=\"center\"/>\n",
    "<br>\n",
    "<img src=\"figures/triple_seasonality_dwy/dynamic_reg_residu.png\" align=\"center\"/>\n",
    "\n",
    "A la hora de interpretar los residuos de este modelo, como con otras técnicas de regresión, lo principal es que los residuos tengan una distribución, normal, aleatoria o de *ruido blanco* como se conoce comúnmente en el campo de procesado de señales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name=\"9\"></a> 9 - Método V: TBATS\n",
    "\n",
    "Alternativa a la Regresión armónica dinámica con términos de Fourier. Está desarrollada por De Livera, Hyndman, & Snyder (https://robjhyndman.com/publications/complex-seasonality/).\n",
    "\n",
    "La función **tbats()** del paquete forecast de R está completamente automatizada, aunque el usuario tiene cierta libertad para seleccionar algunos parámetros de entrada. La elección de estos parámetros reduce el número de combinaciones posible y por lo tanto disminuye el número total de modelos a elegir, por lo que la búsqueda del mejor modelo será más eficiente.\n",
    "\n",
    "Cabe destacar la posibilidad de pasar a la función el parámetro **use.parallel=TRUE**. Con esta opción seleccionada tbats() aprovechará la capacidad multi-hilo del procesador, con la consecuente reducción del tiempo de cálculo.\n",
    "\n",
    "```r\n",
    ">>> train_ts %>% \n",
    "      tbats(use.trend = FALSE, use.arma.errors = TRUE,\n",
    "            use.parallel = TRUE) -> fit2\n",
    "\n",
    "# Forecast and Plot:\n",
    ">>> fc2 <- forecast(fit2, h=24*245, level = 80)\n",
    ">>> autoplot(train_ts) +\n",
    "      autolayer(test_ts, series=\"Test\") +\n",
    "      autolayer(fc2, series=\"TBATS\", PI=FALSE, alpha=0.7) +\n",
    "      ggtitle(\"Power Demand (MW) over time [2016 - 2018]\") +\n",
    "      xlab(\"Year\") + ylab(\"Demand in MW\") +\n",
    "      guides(colour=guide_legend(title=\"Forecast\"))\n",
    "```\n",
    "> **\"TBATS MAPE: 1.22 %\"**\n",
    "\n",
    "<img src=\"figures/triple_seasonality_dwy/tbats.png\" align=\"center\"/>\n",
    "<br>\n",
    "<img src=\"figures/triple_seasonality_dwy/tbats_zoom.png\" align=\"center\"/>\n",
    "<br>\n",
    "<img src=\"figures/triple_seasonality_dwy/tbats_residu.png\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.1 - Resumen de los resultados\n",
    "\n",
    "| Model | MAPE (%) |\n",
    "|---|---|\n",
    "| Holt-Winters | 13.11 |\n",
    "| Prophet | 9.06 |\n",
    "| Mean | 14.7 |\n",
    "| Naïve | 15.18 |\n",
    "| Naïve with drift | 15.04 |\n",
    "| Seasonal naïve | 13.12 |\n",
    "| Dynamic harmonic Reg. + Fourier | 1.04 |\n",
    "| TBATS | 1.22 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name=\"10\"></a> 10 - Conclusiones\n",
    "\n",
    "Del análisis presentado en este trabajo he podido sacar las siguientes valoraciones:\n",
    "<br>\n",
    "- El modelo de Holt-Winters, es un modelo que no está pensado para tratar series temporales con estacionalidad múltiple, aún así ha parecido sacar una solución bastante robusta.\n",
    "<br><br>\n",
    "- La librería Prophet de Facebook, ofrece de manera sencilla una doble API (en Python y R) y muy potente un modelo de predicción con variables de estacionalidad por Transformadas de Fourier. El modelo ha resultado ser muy robusto, sin apenas haber experimentado con los parámetros, y además ofrece una gran flexibilidad, añadir un gran número de estacionalidades de diferentes frecuencias, funciona con datos intra día automáticamente si se le inyecta la serie temporal en forma de dataframe pandas con un índice de tipo  Datetime.\n",
    "<br><br>\n",
    "- El paquete forecast de R otorga al usuario la posibilidad de trabajar de forma muy potente con gran cantidad de modelos, transformacioens y utilidades para analizar y predecir series temporales. Al igual que con la librería Prophet, la experiencia y el tiempo son factores clave para seleccionar los parámetros adecuados para cada serie temporal.\n",
    "<br><br>\n",
    "- Se ha podido observar durante la realización de este trabajo la gran cantidad de factores y parámetros que hay que tener en cuenta para ajustar los modelos de predicción a una serie temporal. Por esta razón modelos como el de Facebook, cuyos valores han sido elegidos en base a ejemplos en la documentación, son con un gran alto de probabilidad subóptimos. Es muy notable la diferencia con los modelos automáticos implementados desde el paquete forecast de R. La selección automática a través de la minimización del **AICc** ha dado lugar a la obtención de modelos más indicados para la serie temporal que se estaba analizando."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name=\"11\"></a> 11 - Futuras líneas de trabajo\n",
    "\n",
    "- En este proyecto se ha considerado una serie temporal univariante, sin ningún tipo de serie adicional variable complementaria ni covariantes (por ejemplo variables climatológicas como la temperatura, horas de sol, etc). Este es uno de los puntos débiles de este análisis, porque es precisamente la fuerte correlación entre el tiempo y el consumo eléctrico uno de los puntos fuertes de los modelos de demanda de electricidad. Este comportamiento se ha visto reflejado durante el EDA, cuando los patrones de gasto eléctrico diferían mucho entre las estaciones del año.\n",
    "<br><br>\n",
    "- Los fines de semana, los periodos estivales (cuando es mayor la cantidad de gente de vacaciones), los eventos de un día afectan en gran medida al consumo. Así como la propia localización de los datos. No es lo mismo tomar datos de demanda eléctrica en una urbe, que en una zona rural, o en una zona turística donde se esperaran aún mayor varianza entre los periodos de vacaciones y el resto del año. Por todos estos factores es muy común la presencia de outliers presentes en los datos en forma de picos de gasto o días de muy poco consumo. Este es sin duda uno de los puntos fuerte de la **Prohet** de Facebook, que es la facilidad para modelizar este tipo de casos especiales.\n",
    "<br><br>\n",
    "- Automatizar la selección del hiperparámetro K para los modelos en los que se modelizan regresores externos de estacionalidad mediante series de Fourier. Implementando un pequeño Grid Search entre diferentes valores de K, y tomando como condición una minimización del valor del **AICc** podría dar lugar a mejores predicciones.\n",
    "<br><br>\n",
    "- Por último, pienso que sería interesante automatizar una validación cruzada con diferentes rangos de la serie temporal. De esta forma se podrían ejectuar diferentes modelos con diferentes rangos de datos y obtener finalmente un modelo más robusto.\n",
    "\n",
    "La implementación de la validación cruzada podría partir de este ciclo junto con el fragmento de código que hay en el Anexo:\n",
    "\n",
    "```python\n",
    "for train_index, test_index in tscv.split(X):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index] # pandas doesn't support directly integer based slicing, so that .iloc is mandatory\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    X_train, y_train = model_forecast.fit()\n",
    "    y_pred = model_forecast.predict(X_test) # in literature is usually seen y_pred as y_hat\n",
    "    cross_validaiton_result(y_test, y_pred)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name=\"12\"></a> 12 - Referencias\n",
    "\n",
    "- Hyndman, R.J., & Athanasopoulos, G. (2018) Forecasting: principles and practice, 2nd edition, OTexts: Melbourne, Australia. https://otexts.com/fpp2/\n",
    "<br><br>\n",
    "- Practical Time Series Forecasting with R: A Hands-On Guid, Galit Shmueli, Kenneth C. Lichtendahl\n",
    "<br><br>\n",
    "- scikit-learn documentation: https://scikit-learn.org/stable/documentation.html\n",
    "<br><br>\n",
    "- StatsModels documentation: https://www.statsmodels.org/stable/index.html\n",
    "<br><br>\n",
    "- Prophet Python API documentation: https://facebook.github.io/prophet/docs/quick_start.html#python-api\n",
    "<br><br>\n",
    "- Forecast Package Documentation: https://pkg.robjhyndman.com/forecast/index.html\n",
    "<br><br>\n",
    "- Kaggle Dataset *Hourly Energy Consumption: Over 10 years of hourly energy consumption data from PJM in Megawatts*: https://www.kaggle.com/robikscube/hourly-energy-consumption"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name=\"anexo\"></a> Anexo: Validación cruzada anidada para series temporales\n",
    "\n",
    "En el análisis de series temporales es muy importante tener en cuenta la ordinalidad de los datos. Es precisamente ésta la propiedad característica de una serie temporal, la relación de cada punto con el todo el histórico de datos anterior a él.\n",
    "\n",
    "Durante este proyecto, y por limitaciones de tiempo y capacidad de computación, he realizado las predicciones y los modelos con una división simple en **un único** conjunto de entrenamiento y un cojunto de **validación**. Sin embargo existe un procedimiento más sofisticado, que es la **validación cruzada anidada**, que podemos visualiar en la imagen siguiente:\n",
    "<br><br>\n",
    "<img src=\"figures/fXZ6k.png\" align=\"center\"/>\n",
    "\n",
    "En análisis de series temporales la proporción de datos incluída en los conjuntos de entrenamiento y validación suele ser del 80%-20%. Aunque suele haber desviaciones respecto estas cantidades dependiendo de la duración de la serie temporal, las características de estacionalidad y estacionareidad, variables covariantes, etc.\n",
    "\n",
    "A continuación he dejado un pequeño fragmento de código que aprovecha la funcionalidad de la clase **TimeSeriesSplit()** del paquete **scikit-learn** para implementar una validación cruzada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cv_indices(cv, X, y, ax, n_splits, lw=10, cmap_cv=plt.cm.coolwarm):\n",
    "    \"\"\"Create a sample plot for indices of a cross-validation object.\"\"\"\n",
    "\n",
    "    # Generate the training/testing visualizations for each CV split\n",
    "    for ii, (tr, tt) in enumerate(cv.split(X=X)):\n",
    "        # Fill in indices with the training/test groups\n",
    "        indices = np.array([np.nan] * len(X))\n",
    "        indices[tt] = 1\n",
    "        indices[tr] = 0\n",
    "\n",
    "        # Visualize the results\n",
    "        ax.scatter(range(len(indices)), [ii + .5] * len(indices),\n",
    "                   c=indices, marker='_', lw=lw, cmap=cmap_cv,\n",
    "                   vmin=-.2, vmax=1.2)\n",
    "\n",
    "    # Formatting\n",
    "    yticklabels = list(range(1, n_splits+1))\n",
    "    ax.set(yticks=np.arange(n_splits) + .5, yticklabels=yticklabels,\n",
    "           xlabel='Sample index', ylabel=\"CV iteration\",\n",
    "           ylim=[n_splits, -.2])\n",
    "    ax.set_title('{}'.format(type(cv).__name__), fontsize=15)\n",
    "    ax.legend([Patch(color=cmap_cv(.8)), Patch(color=cmap_cv(.02))],\n",
    "              ['Testing set', 'Training set'], loc=(1.02, .8))\n",
    "    # Make the legend fit\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's separate the features_target dataframe in features matrix X and target vector y\n",
    "def separate_features_target(df, target_labels):\n",
    "    \"\"\"\n",
    "    This function takes the features+target DataFrame and splits in the features and target matrices\n",
    "    \"\"\"\n",
    "    X = df.drop(target_labels, axis=1)\n",
    "    y = df[target_labels]\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "X, y = separate_features_target(features_target, 'demand_in_MW')\n",
    "display(X.head())\n",
    "display(y.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(7,4))\n",
    "n_splits = 5 #Number of train/cv/test folds\n",
    "#max_train_size = int(len(features_target)*.67/n_splits)\n",
    "tscv = TimeSeriesSplit(n_splits, max_train_size=None)\n",
    "\n",
    "for train_index, test_index in tscv.split(X):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index] # pandas doesn't support directly integer based slicing, so that .iloc is mandatory\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "plot_cv_indices(tscv, X, y, ax, n_splits)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
